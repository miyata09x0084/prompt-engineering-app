{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot Prompting\n",
    "\n",
    "Provide demonstrations to the model to better steer the it towards the desired results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Loading environment variables\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_completion(messages, model=\"gpt-4o\", temperature=0.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's typical to start with a zero-shot prompt on your task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"WizardLM\"]\n"
     ]
    }
   ],
   "source": [
    "messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": \"Your task is to extract model names from machine learning paper abstracts. Your response is an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"]\",\n",
    "          \"type\": \"text\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"Abstract: Training large language models (LLM) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM model are preferred to outputs from OpenAI ChatGPT. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing large language models. Our codes and generated data are public at <link>\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "\n",
    "response = get_chat_completion(messages)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Prompting\n",
    "\n",
    "\n",
    "**Why few-shot prompting?** -- The idea of the providing example here is that we hope to steer the model better on the types of model names we want to extract. \n",
    "\n",
    "**Structuring the few-shot prompt** -- For readability and reliability, we can leverage user + assistant to structure the examples/demonstrations. The idea here is to leverage the dialogue interface to set expectations for the kind of information and the style of the outputs we desire. \n",
    "\n",
    "**How many demonstrations do you need?** -- The more demonstrations the better. Consider starting with 5-10 examples and regularly evaluate if adding more examples leads to improvements. If you can afford it, some of the more recent models can handle up 100s to 1000s of examples (i.e., many-shot learning). Focus on performance first and then optimize for other things like cost and latency (to do this in general).\n",
    "\n",
    "**Randomize the demonstrations for robustness** -- This check is for ensuring that your overall prompt is robust enough to changes like order of demonstrations. Keep in mind that you will be regularly optimizing this prompt so this is crucial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at what a few-shot prompt looks like for the above task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags: ['LlaMa', 'ChatGPT', 'CCNNs']\n"
     ]
    }
   ],
   "source": [
    "messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": \"Your task is to extract model names from machine learning paper abstracts. Your response is an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"]\",\n",
    "          \"type\": \"text\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"Abstract: Generating talking head videos through a face image and a piece of speech audio still contains many challenges. ie, unnatural head movement, distorted expression, and identity modification. We argue that these issues are mainly because of learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, we explicitly model the connections between audio and different types of motion coefficients individually. Precisely, we present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces. As for the head pose, we design PoseVAE via a conditional VAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the unsupervised 3D keypoints space of the proposed face render, and synthesize the final video. We conducted extensive experiments to demonstrate the superiority of our method in terms of motion and video quality. \"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"Tags: ['SadTalker', 'ExpNet', 'PoseVAE']\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"Abstract: We propose a visibility-aware online 3D scene reconstruction approach from posed monocular videos. In particular, we aim to reconstruct the scene from volumetric features. Unlike previous reconstruction methods which aggregate features for each voxel from input views without considering its visibility, we aim to improve the feature fusion by explicitly inferring its visibility from a similarity matrix, computed from its projected features in each image pair. Following previous works, our model is a coarse-to-fine pipeline including a volume sparsification process. Different from their works which sparsify voxels globally with a fixed occupancy threshold, we perform the sparsification on a local feature volume along each visual ray to preserve at least one voxel per ray for more fine details. The sparse local volume is then fused with a global one for online reconstruction. We further propose to predict TSDF in a coarse-to-fine manner by learning its residuals across scales leading to better TSDF predictions. Experimental results on benchmarks show that our method can achieve superior performance with more scene details. Code is available at: \"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"Tags: ['NA']\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"Abstract: Topological deep learning is a rapidly growing field that pertains to the development of deep learning models for data supported on topological domains such as simplicial complexes, cell complexes, and hypergraphs, which generalize many domains encountered in scientific computations. In this paper, we present a unifying deep learning framework built upon a richer data structure that includes widely adopted topological domains. Specifically, we first introduce combinatorial complexes, a novel type of topological domain. Combinatorial complexes can be seen as generalizations of graphs that maintain certain desirable properties. Similar to hypergraphs, combinatorial complexes impose no constraints on the set of relations. In addition, combinatorial complexes permit the construction of hierarchical higher-order relations, analogous to those found in simplicial and cell complexes. Thus, combinatorial complexes generalize and combine useful traits of both hypergraphs and cell complexes, which have emerged as two promising abstractions that facilitate the generalization of graph neural networks to topological spaces. Second, building upon combinatorial complexes and their rich combinatorial and algebraic structure, we develop a general class of message-passing combinatorial complex neural networks (CCNNs), focusing primarily on attention-based CCNNs. We characterize permutation and orientation equivariances of CCNNs, and discuss pooling and unpooling operations within CCNNs in detail. Third, we evaluate the performance of CCNNs on tasks related to mesh shape analysis and graph learning. Our experiments demonstrate that CCNNs have competitive performance as compared to state-of-the-art deep learning models specifically tailored to the same tasks. Our findings demonstrate the advantages of incorporating higher-order relations into deep learning models in different applications.\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"Tags: ['CCNNs']\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"Abstract: Training large language models (LLM) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM model are preferred to outputs from OpenAI ChatGPT. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing large language models. Our codes and generated data are public at\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"refusal\": False,\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"Tags: ['Evol-Instruct', 'WizardLM', 'ChatGPT', 'LLaMa']\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"Abstract: Two of the most powerful models are LlaMa and ChatGPT. Similar to hypergraphs, combinatorial complexes impose no constraints on the set of relations. In addition, combinatorial complexes permit the construction of hierarchical higher-order relations, analogous to those found in simplicial and cell complexes. Thus, combinatorial complexes generalize and combine useful traits of both hypergraphs and cell complexes, which have emerged as two promising abstractions that facilitate the generalization of graph neural networks to topological spaces. Second, building upon combinatorial complexes and their rich combinatorial and algebraic structure, we develop a general class of message-passing combinatorial complex neural networks (CCNNs), focusing primarily on attention-based CCNNs. \"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_chat_completion(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What demonstrations to use in few-shot\n",
    "\n",
    "A few tips for what to consider when preparing demonstrations for few-shot prompts:\n",
    "\n",
    "- **Provide many input/output pairs (demonstrations) showing the behavior you want**  \n",
    "  - Not every exemplar needs input/output pair; it depends on the task\n",
    "\n",
    "- **Make the demonstrations as relevant and diverse as possible**  \n",
    "  - Using similar demonstrations could also work in some domains  \n",
    "  - Experiment with different formats and styles but try to use common formats (e.g., Q:A)\n",
    "\n",
    "- **Make sure to account for label distribution**  \n",
    "  - Generally go for a balanced distribution or base it on your data distribution  \n",
    "  - Aim for high-quality, properly labeled exemplars  \n",
    "\n",
    "- **Experiment with roles and different format and styles**\n",
    "  - Leverage user + assistant roles to structure few-shot demonstrations and combine this with explicit indicators where possible\n",
    "  - Use delimiters when adding demonstrations to system prompt to structure them better\n",
    "\n",
    "- **Pay attention to ordering of exemplars as it can affect the results**  \n",
    "\n",
    "- **Cover failure cases/edge cases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the few-shot examples as a template to easily reuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/abstracts.json', 'r') as f:\n",
    "    abstracts = json.load(f)\n",
    "\n",
    "few_shot_examples = [] \n",
    "\n",
    "# append the abstracts list to the few_shot_examples\n",
    "for abstract in abstracts:\n",
    "    # Add user message (abstract)\n",
    "    few_shot_examples.append([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Abstract: \" + abstract['abstract']\n",
    "            }\n",
    "        ]\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": str(abstract['tags'])\n",
    "            }\n",
    "        ]\n",
    "    }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Abstract: Generating talking head videos through a face image and a piece of speech audio still contains many challenges. ie, unnatural head movement, distorted expression, and identity modification. We argue that these issues are mainly because of learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, we explicitly model the connections between audio and different types of motion coefficients individually. Precisely, we present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces. As for the head pose, we design PoseVAE via a conditional VAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the unsupervised 3D keypoints space of the proposed face render, and synthesize the final video. We conducted extensive experiments to demonstrate the superiority of our method in terms of motion and video quality.'}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': \"['SadTalker', 'ExpNet', 'PoseVAE']\"}]}],\n",
       " [{'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Abstract: We propose a visibility-aware online 3D scene reconstruction approach from posed monocular videos. In particular, we aim to reconstruct the scene from volumetric features. Unlike previous reconstruction methods which aggregate features for each voxel from input views without considering its visibility, we aim to improve the feature fusion by explicitly inferring its visibility from a similarity matrix, computed from its projected features in each image pair. Following previous works, our model is a coarse-to-fine pipeline including a volume sparsification process. Different from their works which sparsify voxels globally with a fixed occupancy threshold, we perform the sparsification on a local feature volume along each visual ray to preserve at least one voxel per ray for more fine details. The sparse local volume is then fused with a global one for online reconstruction. We further propose to predict TSDF in a coarse-to-fine manner by learning its residuals across scales leading to better TSDF predictions. Experimental results on benchmarks show that our method can achieve superior performance with more scene details. Code is available at:'}]},\n",
       "  {'role': 'assistant', 'content': [{'type': 'text', 'text': \"['NA']\"}]}],\n",
       " [{'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Abstract: Topological deep learning is a rapidly growing field that pertains to the development of deep learning models for data supported on topological domains such as simplicial complexes, cell complexes, and hypergraphs, which generalize many domains encountered in scientific computations. In this paper, we present a unifying deep learning framework built upon a richer data structure that includes widely adopted topological domains. Specifically, we first introduce combinatorial complexes, a novel type of topological domain. Combinatorial complexes can be seen as generalizations of graphs that maintain certain desirable properties. Similar to hypergraphs, combinatorial complexes impose no constraints on the set of relations. In addition, combinatorial complexes permit the construction of hierarchical higher-order relations, analogous to those found in simplicial and cell complexes. Thus, combinatorial complexes generalize and combine useful traits of both hypergraphs and cell complexes, which have emerged as two promising abstractions that facilitate the generalization of graph neural networks to topological spaces. Second, building upon combinatorial complexes and their rich combinatorial and algebraic structure, we develop a general class of message-passing combinatorial complex neural networks (CCNNs), focusing primarily on attention-based CCNNs. We characterize permutation and orientation equivariances of CCNNs, and discuss pooling and unpooling operations within CCNNs in detail. Third, we evaluate the performance of CCNNs on tasks related to mesh shape analysis and graph learning. Our experiments demonstrate that CCNNs have competitive performance as compared to state-of-the-art deep learning models specifically tailored to the same tasks. Our findings demonstrate the advantages of incorporating higher-order relations into deep learning models in different applications.'}]},\n",
       "  {'role': 'assistant', 'content': [{'type': 'text', 'text': \"['CCNNs']\"}]}],\n",
       " [{'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Abstract: Training large language models (LLM) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM model are preferred to outputs from OpenAI ChatGPT. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing large language models. Our codes and generated data are public at'}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': \"['Evol-Instruct', 'WizardLM', 'ChatGPT', 'LLaMa']\"}]}]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Prompt: \n",
      " [{'role': 'system', 'content': [{'text': 'Your task is to extract model names from machine learning paper abstracts. Your response is an array of the model names in the format [\"model_name\"]. If you don\\'t find model names in the abstract or you are not sure, return [\"NA\"]', 'type': 'text'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Abstract: Generating talking head videos through a face image and a piece of speech audio still contains many challenges. ie, unnatural head movement, distorted expression, and identity modification. We argue that these issues are mainly because of learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, we explicitly model the connections between audio and different types of motion coefficients individually. Precisely, we present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces. As for the head pose, we design PoseVAE via a conditional VAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the unsupervised 3D keypoints space of the proposed face render, and synthesize the final video. We conducted extensive experiments to demonstrate the superiority of our method in terms of motion and video quality.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': \"['SadTalker', 'ExpNet', 'PoseVAE']\"}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Abstract: We propose a visibility-aware online 3D scene reconstruction approach from posed monocular videos. In particular, we aim to reconstruct the scene from volumetric features. Unlike previous reconstruction methods which aggregate features for each voxel from input views without considering its visibility, we aim to improve the feature fusion by explicitly inferring its visibility from a similarity matrix, computed from its projected features in each image pair. Following previous works, our model is a coarse-to-fine pipeline including a volume sparsification process. Different from their works which sparsify voxels globally with a fixed occupancy threshold, we perform the sparsification on a local feature volume along each visual ray to preserve at least one voxel per ray for more fine details. The sparse local volume is then fused with a global one for online reconstruction. We further propose to predict TSDF in a coarse-to-fine manner by learning its residuals across scales leading to better TSDF predictions. Experimental results on benchmarks show that our method can achieve superior performance with more scene details. Code is available at:'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': \"['NA']\"}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Abstract: Topological deep learning is a rapidly growing field that pertains to the development of deep learning models for data supported on topological domains such as simplicial complexes, cell complexes, and hypergraphs, which generalize many domains encountered in scientific computations. In this paper, we present a unifying deep learning framework built upon a richer data structure that includes widely adopted topological domains. Specifically, we first introduce combinatorial complexes, a novel type of topological domain. Combinatorial complexes can be seen as generalizations of graphs that maintain certain desirable properties. Similar to hypergraphs, combinatorial complexes impose no constraints on the set of relations. In addition, combinatorial complexes permit the construction of hierarchical higher-order relations, analogous to those found in simplicial and cell complexes. Thus, combinatorial complexes generalize and combine useful traits of both hypergraphs and cell complexes, which have emerged as two promising abstractions that facilitate the generalization of graph neural networks to topological spaces. Second, building upon combinatorial complexes and their rich combinatorial and algebraic structure, we develop a general class of message-passing combinatorial complex neural networks (CCNNs), focusing primarily on attention-based CCNNs. We characterize permutation and orientation equivariances of CCNNs, and discuss pooling and unpooling operations within CCNNs in detail. Third, we evaluate the performance of CCNNs on tasks related to mesh shape analysis and graph learning. Our experiments demonstrate that CCNNs have competitive performance as compared to state-of-the-art deep learning models specifically tailored to the same tasks. Our findings demonstrate the advantages of incorporating higher-order relations into deep learning models in different applications.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': \"['CCNNs']\"}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Abstract: Training large language models (LLM) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM model are preferred to outputs from OpenAI ChatGPT. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing large language models. Our codes and generated data are public at'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': \"['Evol-Instruct', 'WizardLM', 'ChatGPT', 'LLaMa']\"}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Abstract: We introduce PokerBench - a benchmark for evaluating the poker-playing abilities of large language models (LLMs). As LLMs excel in traditional NLP tasks, their application to complex, strategic games like poker poses a new challenge. Poker, an incomplete information game, demands a multitude of skills such as mathematics, reasoning, planning, strategy, and a deep understanding of game theory and human psychology. This makes Poker the ideal next frontier for large language models. PokerBench consists of a comprehensive compilation of 11,000 most important scenarios, split between pre-flop and post-flop play, developed in collaboration with trained poker players. We evaluate prominent models including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models, finding that all state-of-the-art LLMs underperform in playing optimal poker. However, after fine-tuning, these models show marked improvements. We validate PokerBench by having models with different scores compete with each other, demonstrating that higher scores on PokerBench lead to higher win rates in actual poker games. Through gameplay between our fine-tuned model and GPT-4, we also identify limitations of simple supervised fine-tuning for learning optimal playing strategy, suggesting the need for more advanced methodologies for effectively training language models to excel in games. PokerBench thus presents a unique benchmark for a quick and reliable evaluation of the poker-playing ability of LLMs as well as a comprehensive benchmark to study the progress of LLMs in complex game-playing scenarios. '}]}] \n",
      "\n",
      "\n",
      "Response: \n",
      " ['PokerBench', 'GPT-4', 'ChatGPT 3.5', 'Llama', 'Gemma']\n"
     ]
    }
   ],
   "source": [
    "system_message = [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": \"Your task is to extract model names from machine learning paper abstracts. Your response is an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"]\",\n",
    "          \"type\": \"text\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "]\n",
    "\n",
    "user_message = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"Abstract: We introduce PokerBench - a benchmark for evaluating the poker-playing abilities of large language models (LLMs). As LLMs excel in traditional NLP tasks, their application to complex, strategic games like poker poses a new challenge. Poker, an incomplete information game, demands a multitude of skills such as mathematics, reasoning, planning, strategy, and a deep understanding of game theory and human psychology. This makes Poker the ideal next frontier for large language models. PokerBench consists of a comprehensive compilation of 11,000 most important scenarios, split between pre-flop and post-flop play, developed in collaboration with trained poker players. We evaluate prominent models including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models, finding that all state-of-the-art LLMs underperform in playing optimal poker. However, after fine-tuning, these models show marked improvements. We validate PokerBench by having models with different scores compete with each other, demonstrating that higher scores on PokerBench lead to higher win rates in actual poker games. Through gameplay between our fine-tuned model and GPT-4, we also identify limitations of simple supervised fine-tuning for learning optimal playing strategy, suggesting the need for more advanced methodologies for effectively training language models to excel in games. PokerBench thus presents a unique benchmark for a quick and reliable evaluation of the poker-playing ability of LLMs as well as a comprehensive benchmark to study the progress of LLMs in complex game-playing scenarios. \"\n",
    "        }\n",
    "    ]\n",
    "}]\n",
    "\n",
    "messages = system_message + [item for sublist in few_shot_examples for item in sublist] + user_message\n",
    "\n",
    "print(\"Full Prompt: \\n\", messages, \"\\n\\n\")\n",
    "\n",
    "response = get_chat_completion(messages)\n",
    "\n",
    "print(\"Response: \\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the prompt robustness, you can try randomizing the order of the few shot examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Abstract: Generating talking head videos through a face image and a piece of speech audio still contains many challenges. ie, unnatural head movement, distorted expression, and identity modification. We argue that these issues are mainly because of learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, we explicitly model the connections between audio and different types of motion coefficients individually. Precisely, we present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces. As for the head pose, we design PoseVAE via a conditional VAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the unsupervised 3D keypoints space of the proposed face render, and synthesize the final video. We conducted extensive experiments to demonstrate the superiority of our method in terms of motion and video quality.'}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': \"['SadTalker', 'ExpNet', 'PoseVAE']\"}]}],\n",
       " [{'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Abstract: Training large language models (LLM) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM model are preferred to outputs from OpenAI ChatGPT. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing large language models. Our codes and generated data are public at'}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': \"['Evol-Instruct', 'WizardLM', 'ChatGPT', 'LLaMa']\"}]}],\n",
       " [{'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Abstract: We propose a visibility-aware online 3D scene reconstruction approach from posed monocular videos. In particular, we aim to reconstruct the scene from volumetric features. Unlike previous reconstruction methods which aggregate features for each voxel from input views without considering its visibility, we aim to improve the feature fusion by explicitly inferring its visibility from a similarity matrix, computed from its projected features in each image pair. Following previous works, our model is a coarse-to-fine pipeline including a volume sparsification process. Different from their works which sparsify voxels globally with a fixed occupancy threshold, we perform the sparsification on a local feature volume along each visual ray to preserve at least one voxel per ray for more fine details. The sparse local volume is then fused with a global one for online reconstruction. We further propose to predict TSDF in a coarse-to-fine manner by learning its residuals across scales leading to better TSDF predictions. Experimental results on benchmarks show that our method can achieve superior performance with more scene details. Code is available at:'}]},\n",
       "  {'role': 'assistant', 'content': [{'type': 'text', 'text': \"['NA']\"}]}],\n",
       " [{'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Abstract: Topological deep learning is a rapidly growing field that pertains to the development of deep learning models for data supported on topological domains such as simplicial complexes, cell complexes, and hypergraphs, which generalize many domains encountered in scientific computations. In this paper, we present a unifying deep learning framework built upon a richer data structure that includes widely adopted topological domains. Specifically, we first introduce combinatorial complexes, a novel type of topological domain. Combinatorial complexes can be seen as generalizations of graphs that maintain certain desirable properties. Similar to hypergraphs, combinatorial complexes impose no constraints on the set of relations. In addition, combinatorial complexes permit the construction of hierarchical higher-order relations, analogous to those found in simplicial and cell complexes. Thus, combinatorial complexes generalize and combine useful traits of both hypergraphs and cell complexes, which have emerged as two promising abstractions that facilitate the generalization of graph neural networks to topological spaces. Second, building upon combinatorial complexes and their rich combinatorial and algebraic structure, we develop a general class of message-passing combinatorial complex neural networks (CCNNs), focusing primarily on attention-based CCNNs. We characterize permutation and orientation equivariances of CCNNs, and discuss pooling and unpooling operations within CCNNs in detail. Third, we evaluate the performance of CCNNs on tasks related to mesh shape analysis and graph learning. Our experiments demonstrate that CCNNs have competitive performance as compared to state-of-the-art deep learning models specifically tailored to the same tasks. Our findings demonstrate the advantages of incorporating higher-order relations into deep learning models in different applications.'}]},\n",
       "  {'role': 'assistant', 'content': [{'type': 'text', 'text': \"['CCNNs']\"}]}]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PokerBench', 'GPT-4', 'ChatGPT 3.5', 'Llama', 'Gemma']\n"
     ]
    }
   ],
   "source": [
    "messages = system_message + [item for sublist in few_shot_examples for item in sublist] + user_message\n",
    "\n",
    "response = get_chat_completion(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Few-shot Prompting\n",
    "\n",
    "The goal is to compare our new few-shot prompt to see how much we are improving compared with the zero-shot base prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import val_data.json\n",
    "import json\n",
    "\n",
    "with open('data/val_data.json', 'r') as f:\n",
    "    val_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paper': 'Grandmaster-Level Chess Without Search', 'abstract': \"The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. This paper investigates the impact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter transformer model with supervised learning on a dataset of 10 million chess games. We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our model outperforms AlphaZero's policy and value networks (without MCTS) and GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size shows that strong chess performance only arises at sufficient scale. To validate our results, we perform an extensive series of ablations of design choices and hyperparameters.\", 'gold_labels': \"['AlphaZero', ' GPT-3.5-turbo-instruct']\"}\n"
     ]
    }
   ],
   "source": [
    "print(val_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"['AlphaCodium', 'GPT-4']\"]\n",
      "['[\"AlphaCodium\"]']\n"
     ]
    }
   ],
   "source": [
    "# helps to format the user message\n",
    "def format_user_message(paper):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Abstract: \" + paper[\"abstract\"]}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "def get_zero_shot_predictions(val_data, system_message):\n",
    "    \"\"\"Calls the model with the system message and returns the predictions\"\"\"\n",
    "    predictions = []\n",
    "    for paper in val_data:\n",
    "        messages = system_message + format_user_message(paper)\n",
    "        response = get_chat_completion(messages)\n",
    "        predictions.append(response)\n",
    "    return predictions\n",
    "\n",
    "# helps to get the few_shot_predictions\n",
    "def get_few_shot_predictions(val_data, few_shot_examples, system_message):\n",
    "    \"\"\"Calls the model with the few shot examples and returns the predictions\"\"\"\n",
    "    predictions = []\n",
    "    for paper in val_data:\n",
    "        messages = system_message + few_shot_examples + format_user_message(paper)\n",
    "        response = get_chat_completion(messages)\n",
    "        predictions.append(response)\n",
    "    return predictions\n",
    "\n",
    "final_few_shot_examples =[item for sublist in few_shot_examples for item in sublist]\n",
    "\n",
    "# testing it with a paper\n",
    "few_shot_predictions = get_few_shot_predictions([val_data[31]],  final_few_shot_examples, system_message)\n",
    "zero_shot_predictions = get_zero_shot_predictions([val_data[31]], system_message)\n",
    "\n",
    "print(few_shot_predictions)\n",
    "print(zero_shot_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPROVEMENT: Maybe might be good to validate results or even use structured outputs for this use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the predictions using both zero-shot prompting and few-shot prompting (takes about 3 mins to run, ~$0.20 at the time of recording):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run both the zero shot and few shot predictions for all the papers\n",
    "zero_shot_predictions = get_zero_shot_predictions(val_data, system_message)\n",
    "few_shot_predictions = get_few_shot_predictions(val_data, final_few_shot_examples, system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval the output of the zero shot predictions\n",
    "zero_shot_predictions = [eval(prediction) for prediction in zero_shot_predictions]\n",
    "# eval the output of the few shot predictions\n",
    "few_shot_predictions = [eval(prediction) for prediction in few_shot_predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the predictions for easy reuse later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you want export the predictions as a json file for future use\n",
    "#with open('data/predictions.json', 'w') as f:\n",
    "#    json.dump({'zero_shot_predictions': zero_shot_predictions, 'few_shot_predictions': few_shot_predictions}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the results the next time you run the notebook (uncomment if you need it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the predictions from the json file\n",
    "#with open('data/predictions.json', 'r') as f:\n",
    "#    predictions = json.load(f)\n",
    "#zero_shot_predictions = predictions['zero_shot_predictions']\n",
    "#few_shot_predictions = predictions['few_shot_predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval the output of the zero shot predictions\n",
    "#zero_shot_predictions = [eval(prediction) for prediction in zero_shot_predictions]\n",
    "# eval the output of the few shot predictions\n",
    "#few_shot_predictions = [eval(prediction) for prediction in few_shot_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NA'],\n",
       " ['AnyTool', 'ToolLLM', 'GPT-4'],\n",
       " ['NA'],\n",
       " ['NA'],\n",
       " ['ALOHA 2', 'ALOHA'],\n",
       " ['NA'],\n",
       " ['SELF-DISCOVER'],\n",
       " ['DeepSeekMath 7B', 'DeepSeek-Coder-Base-v1.5 7B', 'Gemini-Ultra', 'GPT-4'],\n",
       " ['NA'],\n",
       " ['NA'],\n",
       " ['OLMo'],\n",
       " ['NA'],\n",
       " ['CRAG'],\n",
       " ['NA'],\n",
       " ['NA'],\n",
       " ['MoE-Tuning', 'MoE-LLaVA', 'LLaVA-1.5-7B', 'LLaVA-1.5-13B'],\n",
       " ['WRAP'],\n",
       " ['Retrieval-Augmented Generation (RAG)'],\n",
       " ['NA'],\n",
       " ['SliceGPT', 'LLAMA2-70B', 'OPT 66B', 'Phi-2'],\n",
       " ['Depth Anything'],\n",
       " ['Llama-2', 'MPT', 'OpenLLaMA'],\n",
       " ['MambaByte'],\n",
       " ['DreamPaint', 'Diffuse to Choose'],\n",
       " ['WARM'],\n",
       " ['NA'],\n",
       " ['RTVLM', 'LLaVA-v1.5'],\n",
       " ['Lumiere'],\n",
       " ['Medusa', 'Medusa-1', 'Medusa-2'],\n",
       " ['AgentBoard'],\n",
       " ['AlphaGeometry'],\n",
       " ['AlphaCodium'],\n",
       " ['Llama2-13B', 'GPT-3.5', 'GPT-4'],\n",
       " ['Self-Rewarding Language Models', 'Llama 2 70B'],\n",
       " ['proxy-tuning', 'Llama2-70B'],\n",
       " ['ReFT'],\n",
       " ['NA'],\n",
       " ['Patchscopes'],\n",
       " ['QLoRA'],\n",
       " ['Mamba', 'MoE-Mamba', 'Transformer-MoE'],\n",
       " ['InseRF'],\n",
       " ['NA'],\n",
       " ['ChatGPT'],\n",
       " ['MagicVideo-V2',\n",
       "  'Runway',\n",
       "  'Pika 1.0',\n",
       "  'Morph',\n",
       "  'Moon Valley',\n",
       "  'Stable Video Diffusion'],\n",
       " ['TrustLLM'],\n",
       " ['Chain-of-Table'],\n",
       " ['Llama 2-7b Chat', 'GPT-3.5', 'GPT-4'],\n",
       " ['RAISE'],\n",
       " ['LLaMA-2-13B', 'FormatSpread'],\n",
       " ['NA'],\n",
       " ['Retrieval Augmented Generation', 'Knowledge Retrieval', 'CoNLI', 'CoVe'],\n",
       " ['Mobile ALOHA'],\n",
       " ['SPIN'],\n",
       " ['LLaMA Pro-8.3B', 'LLaMA Pro', 'LLaMA Pro-Instruct'],\n",
       " ['CALM', 'PaLM2-S'],\n",
       " ['Mixtral-8x7B'],\n",
       " ['GPT-4V(ision)', 'Gemini', 'SEEACT', 'GPT-4', 'FLAN-T5', 'BLIP-2'],\n",
       " ['DocLLM'],\n",
       " ['NA'],\n",
       " ['instruct-imagen']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get actual tags\n",
    "actual_tags = [eval(paper['gold_labels']) for paper in val_data]\n",
    "\n",
    "# clean up: remove white spaces from the items in the arrays for the actual tags\n",
    "actual_tags = [[item.strip() for item in tag] for tag in actual_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AlphaZero', 'GPT-3.5-turbo-instruct'],\n",
       " ['AnyTool', 'GPT-4', 'ToolLLM'],\n",
       " ['NA'],\n",
       " ['GPT-3.5-turbo', 'Gemini-pro']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_tags[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare predictions with the actual tags using LLM-as-a-judge\n",
    "def get_llm_as_a_judge_predictions(predictions, actual_tags):\n",
    "    \"\"\"Calls the model with the predictions and returns the predictions\"\"\"\n",
    "\n",
    "    final_assessment = []\n",
    "\n",
    "    for prediction, actual_tag in zip(predictions, actual_tags):\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a judge that will compare the predictions with the actual tags and return either 'correct' or 'incorrect' for each prediction. The predictions are an array of model names and the actual tags are an array of model names. The predictions and actual tags don't need to be in the same order to be correct as long as the correct model names are present in both the predictions and the actual tags.\"}]\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": f\"Predictions: {prediction} Actual Tags: {actual_tag}\"}]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = get_chat_completion(messages)\n",
    "        final_assessment.append(response)\n",
    "\n",
    "\n",
    "    return final_assessment\n",
    "\n",
    "# assess a few predictions\n",
    "assessment = get_llm_as_a_judge_predictions(zero_shot_predictions[0:10], actual_tags[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run assessment for both types of predictions (takes less than a minute; ~$0.04):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run assessment for zero shot predictions\n",
    "zero_shot_assessment = get_llm_as_a_judge_predictions(zero_shot_predictions, actual_tags)\n",
    "# run assessment for few shot predictions\n",
    "few_shot_assessment = get_llm_as_a_judge_predictions(few_shot_predictions, actual_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot accuracy: 0.6\n",
      "Few-shot accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "# count the number of correct predictions\n",
    "zero_shot_correct = zero_shot_assessment.count('correct')\n",
    "few_shot_correct = few_shot_assessment.count('correct')\n",
    "\n",
    "print(f\"Zero-shot accuracy: {zero_shot_correct/len(zero_shot_assessment)}\")\n",
    "print(f\"Few-shot accuracy: {few_shot_correct/len(few_shot_assessment)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a lot of improvement but we can continue to optimize the system prompt and take a closer look at the LLM-as-a-Judge and make it less strict.\n",
    "\n",
    "The good news is that we now have a way to systematically test any improvements on our few-shot prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other things to try:\n",
    "- Optimize better the system prompt\n",
    "- Use the o1-mini model to the LLM-as-a-judge evaluation\n",
    "- Expand the few-shot prompt examples to more edge cases but perform the error analysis first"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pe-for-devs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
