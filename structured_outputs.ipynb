{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs for LLMs\n",
    "\n",
    "This guide demonstrates how to use structured outputs with LLMs and provides a practical implementation to further understand the topic.\n",
    "\n",
    "## What are Structured Outputs?\n",
    "\n",
    "Structured outputs enable LLMs to generate responses that adhere to a specific JSON schema (this can be provided using Pydantic). This ensures that:\n",
    "\n",
    "1. **Responses are consistently formatted** - The model returns data in a predictable structure\n",
    "2. **Type-safety is maintained** - No need to validate or retry incorrectly formatted responses\n",
    "3. **Integration with applications is simplified** - Data can be directly fed into downstream processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from utils import *\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Use Cases\n",
    "\n",
    "- Tasks that require reasoning (step-by-step outputs)\n",
    "- Unstructured data --> structured data\n",
    "- UI Generation (e.g., generate valid HTML)\n",
    "- Content moderation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MathReasoning(steps=[Step(explanation='Start with the equation: \\\\(8x + 7 = -23\\\\)', output='8x + 7 = -23'), Step(explanation='Subtract 7 from both sides to isolate the term with \\\\(x\\\\) on the left side. This gives you: \\\\(8x + 7 - 7 = -23 - 7\\\\).', output='8x = -30'), Step(explanation='Now, divide both sides of the equation by 8 to solve for \\\\(x\\\\).', output='x = \\\\frac{-30}{8}'), Step(explanation='Simplify \\\\(\\\\frac{-30}{8}\\\\) by dividing the numerator and the denominator by their greatest common divisor, which is 2.', output='x = \\\\frac{-15}{4}')], final_answer='x = -\\\\frac{15}{4}')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: str\n",
    "\n",
    "system_message = \"You are a helpful math tutor. Guide the user through the solution step by step.\"\n",
    "query = \"how can I solve 8x + 7 = -23\"\n",
    "\n",
    "get_structured_output(query, system_message, MathReasoning, \"math reasoning\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Data Structure\n",
    "\n",
    "To maximize the quality of model generations, OpenAI recommends the following:\n",
    "\n",
    "- Name keys clearly and intuitively\n",
    "- Create clear titles and descriptions for important keys in your structure\n",
    "- Create and use evals to determine the structure that works best for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Moderation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContentCompliance(is_violating=False, category=None, explanation_if_violating=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from typing import Optional\n",
    "\n",
    "class Category(str, Enum):\n",
    "    violence = \"violence\"\n",
    "    sexual = \"sexual\"\n",
    "    self_harm = \"self_harm\"\n",
    "\n",
    "class ContentCompliance(BaseModel):\n",
    "    is_violating: bool\n",
    "    category: Optional[Category]\n",
    "    explanation_if_violating: Optional[str]\n",
    "\n",
    "system_message = \"Determine if the user input violates specific guidelines and explain if they do.\"\n",
    "query = \"How do I prepare for a job interview?\"\n",
    "\n",
    "get_structured_output(query, system_message, ContentCompliance, \"content compliance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Refusals\n",
    "\n",
    "As with other calls, the structured output with user inputs could lead to refusals, for instance, because of safety reasons. To handle this in your application, you can use a conditional logic to check if the `refusal` property is present in the structured output.\n",
    "\n",
    "If there is a refusal, the output will look different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps=[Step(explanation=\"The goal is to solve for \\\\( x \\\\) in the equation \\\\( 8x + 7 = -23 \\\\). Start by isolating the term with \\\\( x \\\\). Since the constant term on the left side is \\\\( +7 \\\\), we'll subtract 7 from both sides of the equation to remove it.\", output='Perform: \\\\( 8x + 7 - 7 = -23 - 7 \\\\)'), Step(explanation='Subtracting 7 from both sides simplifies to \\\\( 8x = -30 \\\\). Now, we have an equation with the variable term \\\\( 8x \\\\) by itself on one side.', output='Result: \\\\( 8x = -30 \\\\)'), Step(explanation='To solve for \\\\( x \\\\), divide both sides by 8, the coefficient of \\\\( x \\\\). This will isolate \\\\( x \\\\).', output='Perform: \\\\( \\\\frac{8x}{8} = \\\\frac{-30}{8} \\\\)'), Step(explanation='Simplifying \\\\( \\\\frac{8x}{8} \\\\) gives \\\\( x \\\\), and simplifying \\\\( \\\\frac{-30}{8} \\\\) results in \\\\( -\\\\frac{15}{4} \\\\), which is the value of \\\\( x \\\\).', output='Result: \\\\( x = -\\\\frac{15}{4} \\\\)')] final_answer='x = -\\\\frac{15}{4}'\n"
     ]
    }
   ],
   "source": [
    "# users the same schemas defined in the previous CoT example\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n",
    "    ],\n",
    "    response_format=MathReasoning,\n",
    ")\n",
    "\n",
    "math_reasoning = completion.choices[0].message\n",
    "\n",
    "# If the model refuses to respond, you will get a refusal message\n",
    "if (math_reasoning.refusal):\n",
    "    print(math_reasoning.refusal)\n",
    "else:\n",
    "    print(math_reasoning.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Incompatible Inputs & Mistakes\n",
    "\n",
    "For open-ended applications that accept user-generated inputs, make sure to deal with incompatible requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MathReasoning(steps=[Step(explanation=\"The user's question asks about the weather today, which isn't related to solving a math problem. I don't have the capability to provide weather updates or forecasts.\", output=\"The user's question is outside the scope of math problem-solving.\")], final_answer=\"I'm unable to provide weather updates. Please check a reliable weather source.\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"You are a helpful math tutor. Guide the user through the solution step by step.\"\n",
    "query = \"What's the weather today?\"\n",
    "\n",
    "get_structured_output(query, system_message, MathReasoning, \"math reasoning\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will always try to adhere to the schema provided. It can results in hallucinations so to mitigate this issue you can improve the system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MathReasoning(steps=[], final_answer='')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"You are a helpful math tutor. Guide the user through the solution step by step. Return empty parameters if the request is invalid.\"\n",
    "query = \"What's the weather today?\"\n",
    "\n",
    "get_structured_output(query, system_message, MathReasoning, \"math reasoning\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structured outputs feature is not perfect and might require additional tuning of the system prompt/instructions. Breaking the task into simpler subtasks, as we do in CoT, can also help. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the documentation for further tips and best practices: https://platform.openai.com/docs/guides/structured-outputs#supported-schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pe-for-devs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
